{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from pprint import pprint\n",
    "import unicodedata\n",
    "from transformers import AutoModel, BasicTokenizer, BertTokenizerFast\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_state     ori_data\t       pretrained_word_emb  tplinker\r\n",
      "common\t\t     ori_notebooks     raw_data\t\t    tplinker_plus\r\n",
      "data4bert\t     postprocess       README.md\r\n",
      "data4bilstm\t     preprocess        results\r\n",
      "joint_extr.egg-info  pretrained_model  setup.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_home = \"../../pretrained_models\"\n",
    "data_home = \"../ori_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covid19_rel_lianxiangjia  nyt10  nyt_star  webnlg_star\txf_event_extr_t1\r\n",
      "nyt\t\t\t  nyt11  webnlg    wiki_kbp\txf_event_extr_t1_backup\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../ori_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(pretrained_model_home, \"bert-base-cased\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, add_special_tokens = False, do_lower_case = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_home, \"webnlg_star\", \"train_data.json\")\n",
    "valid_path = os.path.join(data_home, \"webnlg_star\", \"valid_data.json\")\n",
    "test_path = os.path.join(data_home, \"webnlg_star\", \"test_data\", \"test_triples.json\")\n",
    "\n",
    "train_data = json.load(open(train_path, \"r\", encoding = \"utf-8\"))\n",
    "valid_data = json.load(open(valid_path, \"r\", encoding = \"utf-8\"))\n",
    "test_data = json.load(open(test_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_set = set()\n",
    "for sample in train_data + valid_data + test_data:\n",
    "    for triplet in sample[\"triple_list\"]:\n",
    "        rel_set.add(triplet[1])\n",
    "len(rel_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1st_runway_LengthFeet',\n",
       " '1st_runway_LengthMetre',\n",
       " '1st_runway_Number',\n",
       " '1st_runway_SurfaceType',\n",
       " '3rd_runway_LengthFeet',\n",
       " '4th_runway_LengthFeet',\n",
       " '5th_runway_Number',\n",
       " 'EISSN_number',\n",
       " 'LCCN_number',\n",
       " 'OCLC_number',\n",
       " 'academicDiscipline',\n",
       " 'academicStaffSize',\n",
       " 'administrativeArrondissement',\n",
       " 'administrativeCounty',\n",
       " 'affiliation',\n",
       " 'affiliations',\n",
       " 'aircraftFighter',\n",
       " 'aircraftHelicopter',\n",
       " 'almaMater',\n",
       " 'anthem',\n",
       " 'architect',\n",
       " 'architecturalStyle',\n",
       " 'areaCode',\n",
       " 'attackAircraft',\n",
       " 'author',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'backup pilot',\n",
       " 'battles',\n",
       " 'bedCount',\n",
       " 'bird',\n",
       " 'birthPlace',\n",
       " 'broadcastedBy',\n",
       " 'buildingStartDate',\n",
       " 'buildingType',\n",
       " 'capital',\n",
       " 'category',\n",
       " 'chairman',\n",
       " 'champions',\n",
       " 'chancellor',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'city',\n",
       " 'cityServed',\n",
       " 'class',\n",
       " 'club',\n",
       " 'commander',\n",
       " 'compete in',\n",
       " 'completionDate',\n",
       " 'country',\n",
       " 'countySeat',\n",
       " 'creator',\n",
       " 'creatorOfDish',\n",
       " 'crewMembers',\n",
       " 'currency',\n",
       " 'currentTenants',\n",
       " 'dean',\n",
       " 'deathPlace',\n",
       " 'dedicatedTo',\n",
       " 'demonym',\n",
       " 'developer',\n",
       " 'dishVariation',\n",
       " 'district',\n",
       " 'division',\n",
       " 'doctoralAdvisor',\n",
       " 'editor',\n",
       " 'elevationAboveTheSeaLevel',\n",
       " 'elevationAboveTheSeaLevel_(in_feet)',\n",
       " 'elevationAboveTheSeaLevel_(in_metres)',\n",
       " 'established',\n",
       " 'ethnicGroup',\n",
       " 'ethnicGroups',\n",
       " 'family',\n",
       " 'firstPublicationYear',\n",
       " 'floorCount',\n",
       " 'followedBy',\n",
       " 'fossil',\n",
       " 'foundationPlace',\n",
       " 'foundedBy',\n",
       " 'founder',\n",
       " 'gemstone',\n",
       " 'genre',\n",
       " 'genus',\n",
       " 'governingBody',\n",
       " 'governmentType',\n",
       " 'ground',\n",
       " 'has to its north',\n",
       " 'has to its northeast',\n",
       " 'has to its northwest',\n",
       " 'has to its southeast',\n",
       " 'has to its southwest',\n",
       " 'has to its west',\n",
       " 'headquarter',\n",
       " 'headquarters',\n",
       " 'hometown',\n",
       " 'hubAirport',\n",
       " 'influencedBy',\n",
       " 'ingredient',\n",
       " 'isPartOf',\n",
       " 'jurisdiction',\n",
       " 'keyPerson',\n",
       " 'language',\n",
       " 'languages',\n",
       " 'largestCity',\n",
       " 'leader',\n",
       " 'leaderName',\n",
       " 'leaderParty',\n",
       " 'leaderTitle',\n",
       " 'league',\n",
       " 'literaryGenre',\n",
       " 'location',\n",
       " 'locationCity',\n",
       " 'mainIngredients',\n",
       " 'manager',\n",
       " 'mayor',\n",
       " 'mediaType',\n",
       " 'municipality',\n",
       " 'nationality',\n",
       " 'nearestCity',\n",
       " 'neighboringMunicipality',\n",
       " 'nickname',\n",
       " 'notableWork',\n",
       " 'numberOfMembers',\n",
       " 'numberOfPostgraduateStudents',\n",
       " 'numberOfRooms',\n",
       " 'numberOfStudents',\n",
       " 'numberOfUndergraduateStudents',\n",
       " 'occupation',\n",
       " 'officialLanguage',\n",
       " 'operatingOrganisation',\n",
       " 'operator',\n",
       " 'order',\n",
       " 'outlookRanking',\n",
       " 'owner',\n",
       " 'owningOrganisation',\n",
       " 'parentCompany',\n",
       " 'part',\n",
       " 'partsType',\n",
       " 'patronSaint',\n",
       " 'placeOfBirth',\n",
       " 'populationTotal',\n",
       " 'postalCode',\n",
       " 'precededBy',\n",
       " 'president',\n",
       " 'publisher',\n",
       " 'region',\n",
       " 'regionServed',\n",
       " 'religion',\n",
       " 'representative',\n",
       " 'residence',\n",
       " 'river',\n",
       " 'runwayLength',\n",
       " 'season',\n",
       " 'senators',\n",
       " 'served as Chief of the Astronaut Office in',\n",
       " 'significantBuilding',\n",
       " 'significantProject',\n",
       " 'spokenIn',\n",
       " 'sportsGoverningBody',\n",
       " 'sportsOffered',\n",
       " 'starring',\n",
       " 'state',\n",
       " 'tenant',\n",
       " 'title',\n",
       " 'transportAircraft',\n",
       " 'voice',\n",
       " 'was a crew member of',\n",
       " \"was given the 'Technical Campus' status by\",\n",
       " 'was selected by NASA',\n",
       " 'year',\n",
       " 'yearOfConstruction'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki-KBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"wiki_kbp\"\n",
    "data_dir = os.path.join(data_home, experiment_name, \"pre_by_yubowen\")\n",
    "train_data_path = os.path.join(data_dir, \"train.clean.json\")\n",
    "test_data_path = os.path.join(data_dir, \"test.clean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train_data = json.load(open(train_data_path, \"r\", encoding = \"utf-8\"))\n",
    "test_data = json.load(open(test_data_path, \"r\", encoding = \"utf-8\"))\n",
    "random.seed(2333)\n",
    "random.shuffle(ori_train_data)\n",
    "valid_data_size = 5000\n",
    "valid_data, train_data = ori_train_data[:valid_data_size], ori_train_data[valid_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80400 4231 279\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(valid_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_data_dir = os.path.join(data_home, \"nyt\")\n",
    "nyt_train_data_path = os.path.join(nyt_data_dir, \"raw_train.json\")\n",
    "nyt_valid_data_path = os.path.join(nyt_data_dir, \"raw_valid.json\")\n",
    "nyt_test_data_path = os.path.join(nyt_data_dir, \"raw_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    with open(path, \"r\", encoding = \"utf-8\") as train_file:\n",
    "        data = [json.loads(line) for line in train_file]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_train_data = get_data(nyt_train_data_path)\n",
    "nyt_valid_data = get_data(nyt_valid_data_path)\n",
    "nyt_test_data = get_data(nyt_test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentText': 'Here we have a 172-acre island with four-star views of Manhattan , Brooklyn , Ellis Island and the Statue of Liberty .',\n",
       " 'articleId': '/m/vinci8/data1/riedel/projects/relation/kb/nyt1/docstore/nyt-2005-2006.backup/1672681.xml.pb',\n",
       " 'relationMentions': [{'em1Text': 'Ellis Island',\n",
       "   'em2Text': 'Manhattan',\n",
       "   'label': '/location/neighborhood/neighborhood_of'},\n",
       "  {'em1Text': 'Manhattan',\n",
       "   'em2Text': 'Ellis Island',\n",
       "   'label': '/location/location/contains'}],\n",
       " 'entityMentions': [{'start': 0, 'label': 'LOCATION', 'text': 'Manhattan'},\n",
       "  {'start': 1, 'label': 'LOCATION', 'text': 'Brooklyn'},\n",
       "  {'start': 2, 'label': 'LOCATION', 'text': 'Ellis Island'}],\n",
       " 'sentId': '1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_train_data[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66196/66196 [00:00<00:00, 670315.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for sample in tqdm(nyt_train_data + nyt_valid_data + nyt_test_data):\n",
    "    if len(sample[\"relationMentions\"]) == 0:\n",
    "        set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check nested subj and obj\n",
    "# # no\n",
    "# cover_tok = \"[COV]\"\n",
    "# nested_sample_list = []\n",
    "# for sample in tqdm(nyt_train_data + nyt_valid_data + nyt_test_data):\n",
    "#     subj_list = [rel[\"em1Text\"] for rel in sample[\"relationMentions\"] if rel[\"em1Text\"] in sample[\"sentText\"]]\n",
    "#     obj_list = [rel[\"em2Text\"] for rel in sample[\"relationMentions\"]if rel[\"em2Text\"] in sample[\"sentText\"]]\n",
    "#     subj_list = sorted(set(subj_list), key = lambda x: len(x), reverse = True)\n",
    "#     obj_list = sorted(set(obj_list), key = lambda x: len(x), reverse = True)\n",
    "#     text_copy = sample[\"sentText\"][:]\n",
    "#     for ent in subj_list:\n",
    "#         if re.search(ent, text_copy):\n",
    "#             text_copy = re.sub(ent, cover_tok, text_copy)\n",
    "#         else:\n",
    "#             set_trace()\n",
    "#             nested_sample_list.append(sample)\n",
    "    \n",
    "#     text_copy = sample[\"sentText\"][:]\n",
    "#     for ent in obj_list:\n",
    "#         if re.search(ent, text_copy):\n",
    "#             text_copy = re.sub(ent, cover_tok, text_copy)\n",
    "#         else:\n",
    "#             nested_sample_list.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stress_mark(text):\n",
    "    text = \"\".join([c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66196/66196 [00:01<00:00, 44557.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# remove 重音符号\n",
    "for sample in tqdm(nyt_train_data + nyt_valid_data + nyt_test_data):\n",
    "    for rel in sample[\"relationMentions\"]:\n",
    "        rel[\"em1Text\"] = remove_stress_mark(rel[\"em1Text\"])\n",
    "        rel[\"em2Text\"] = remove_stress_mark(rel[\"em2Text\"])\n",
    "        if rel[\"em1Text\"] not in sample[\"sentText\"] or rel[\"em2Text\"] not in sample[\"sentText\"]:\n",
    "            set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66196/66196 [00:16<00:00, 4054.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# # text 长度\n",
    "# sorted_text_list = []\n",
    "# for sample in tqdm(nyt_train_data + nyt_valid_data + nyt_test_data):\n",
    "#     text_tokens = tokenizer.tokenize(sample[\"sentText\"])\n",
    "#     sorted_text_list.append({\n",
    "#         \"tok_num\": len(text_tokens),\n",
    "#         \"tokens\": text_tokens,\n",
    "#         \"text\": sample[\"sentText\"],\n",
    "#     })\n",
    "# sorted_text_list = sorted(sorted_text_list, key = lambda x: x[\"tok_num\"], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查是否有多个token对应一个字符，类似于韩文的情况\n",
    "# # 没有，那么char span到token span的映射就很简单了，可以先打上char span再打上token span\n",
    "# for sample in tqdm(nyt_train_data + nyt_valid_data + nyt_test_data):\n",
    "#     text = sample[\"sentText\"]\n",
    "#     offset_map = tokenizer.encode_plus(text, \n",
    "#                                        return_offsets_mapping = True, \n",
    "#                                        add_special_tokens = False)[\"offset_mapping\"]\n",
    "#     for ind, sp in enumerate(offset_map):\n",
    "#         if ind == 0:\n",
    "#             continue\n",
    "#         if sp[0] == offset_map[ind - 1][0] and sp[1] == offset_map[ind - 1][1]:\n",
    "#             set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocRED_data_dir = os.path.join(data_home, \"DocRED\")\n",
    "train_annotated_data_path = os.path.join(DocRED_data_dir, \"train_annotated.json\")\n",
    "train_data = json.load(open(train_annotated_data_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'name': 'Zest Airways, Inc.', 'pos': [0, 4], 'sent_id': 0, 'type': 'ORG'},\n",
      "  {'name': 'Asian Spirit and Zest Air',\n",
      "   'pos': [10, 15],\n",
      "   'sent_id': 0,\n",
      "   'type': 'ORG'},\n",
      "  {'name': 'AirAsia Zest', 'pos': [6, 8], 'sent_id': 0, 'type': 'ORG'},\n",
      "  {'name': 'AirAsia Zest', 'pos': [19, 21], 'sent_id': 6, 'type': 'ORG'}],\n",
      " [{'name': 'Ninoy Aquino International Airport',\n",
      "   'pos': [4, 8],\n",
      "   'sent_id': 3,\n",
      "   'type': 'LOC'},\n",
      "  {'name': 'Ninoy Aquino International Airport',\n",
      "   'pos': [26, 30],\n",
      "   'sent_id': 0,\n",
      "   'type': 'LOC'}],\n",
      " [{'name': 'Pasay City', 'pos': [31, 33], 'sent_id': 0, 'type': 'LOC'}],\n",
      " [{'name': 'Metro Manila', 'pos': [34, 36], 'sent_id': 0, 'type': 'LOC'}],\n",
      " [{'name': 'Philippines', 'pos': [38, 39], 'sent_id': 0, 'type': 'LOC'},\n",
      "  {'name': 'Philippines', 'pos': [13, 14], 'sent_id': 4, 'type': 'LOC'},\n",
      "  {'name': 'Republic of the Philippines',\n",
      "   'pos': [25, 29],\n",
      "   'sent_id': 5,\n",
      "   'type': 'LOC'}],\n",
      " [{'name': 'Manila', 'pos': [13, 14], 'sent_id': 1, 'type': 'LOC'},\n",
      "  {'name': 'Manila', 'pos': [9, 10], 'sent_id': 3, 'type': 'LOC'}],\n",
      " [{'name': 'Cebu', 'pos': [15, 16], 'sent_id': 1, 'type': 'LOC'}],\n",
      " [{'name': '24', 'pos': [17, 18], 'sent_id': 1, 'type': 'NUM'}],\n",
      " [{'name': '2013', 'pos': [1, 2], 'sent_id': 2, 'type': 'TIME'},\n",
      "  {'name': 'August 16, 2013', 'pos': [1, 5], 'sent_id': 5, 'type': 'TIME'}],\n",
      " [{'name': 'Philippines AirAsia', 'pos': [9, 11], 'sent_id': 2, 'type': 'ORG'}],\n",
      " [{'name': 'Asian Spirit', 'pos': [5, 7], 'sent_id': 4, 'type': 'ORG'}],\n",
      " [{'name': 'Civil Aviation Authority of the Philippines',\n",
      "   'pos': [7, 13],\n",
      "   'sent_id': 5,\n",
      "   'type': 'ORG'},\n",
      "  {'name': 'CAAP', 'pos': [14, 15], 'sent_id': 5, 'type': 'ORG'}],\n",
      " [{'name': 'Zest Air', 'pos': [34, 36], 'sent_id': 5, 'type': 'ORG'},\n",
      "  {'name': 'Zest Air', 'pos': [7, 9], 'sent_id': 6, 'type': 'ORG'}],\n",
      " [{'name': 'a year', 'pos': [2, 4], 'sent_id': 6, 'type': 'NUM'}],\n",
      " [{'name': 'AirAsia', 'pos': [5, 6], 'sent_id': 6, 'type': 'ORG'}],\n",
      " [{'name': 'AirAsia Philippines', 'pos': [5, 7], 'sent_id': 7, 'type': 'ORG'}],\n",
      " [{'name': 'January 2016', 'pos': [8, 10], 'sent_id': 7, 'type': 'TIME'}]]\n"
     ]
    }
   ],
   "source": [
    "pprint(train_data[0][\"vertexSet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'evidence': [0], 'h': 0, 'r': 'P159', 't': 2},\n",
      " {'evidence': [2, 4, 7], 'h': 0, 'r': 'P17', 't': 4},\n",
      " {'evidence': [6, 7], 'h': 12, 'r': 'P17', 't': 4},\n",
      " {'evidence': [0], 'h': 2, 'r': 'P17', 't': 4},\n",
      " {'evidence': [0], 'h': 2, 'r': 'P131', 't': 3},\n",
      " {'evidence': [0], 'h': 4, 'r': 'P150', 't': 3},\n",
      " {'evidence': [0, 3], 'h': 5, 'r': 'P17', 't': 4},\n",
      " {'evidence': [0], 'h': 3, 'r': 'P150', 't': 2},\n",
      " {'evidence': [0, 3], 'h': 3, 'r': 'P131', 't': 4},\n",
      " {'evidence': [0, 3], 'h': 3, 'r': 'P17', 't': 4},\n",
      " {'evidence': [0, 3], 'h': 1, 'r': 'P131', 't': 2},\n",
      " {'evidence': [0, 3], 'h': 1, 'r': 'P17', 't': 4},\n",
      " {'evidence': [4], 'h': 10, 'r': 'P17', 't': 4}]\n"
     ]
    }
   ],
   "source": [
    "pprint(train_data[0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "webnlg_data_dir = os.path.join(data_home, \"webnlg\", \"original\")\n",
    "webnlg_train_data_dir = os.path.join(webnlg_data_dir, \"train\")\n",
    "webnlg_valid_data_dir = os.path.join(webnlg_data_dir, \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "webnlg_train_data_file_path_list = glob.glob(webnlg_train_data_dir + \"/*/*.xml\")\n",
    "webnlg_valid_data_file_path_list = glob.glob(webnlg_valid_data_dir + \"/*/*.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_sample_list(file_path_list):\n",
    "    all_entry_list = []\n",
    "    for file_path in tqdm(file_path_list, desc = \"Loading entries\"):\n",
    "        soup = BeautifulSoup(open(file_path, \"r\", encoding = \"utf-8\"), \"lxml\")\n",
    "        entry_list = soup.select(\"entry\")\n",
    "        all_entry_list.extend(entry_list)\n",
    "\n",
    "    normal_sample_list = []\n",
    "    sent2spo_list = {}\n",
    "    for entry in tqdm(all_entry_list, desc = \"Transforming into normal format\"):\n",
    "        sents = [lex.get_text() for lex in entry.select(\"lex\")]\n",
    "        spo_list = [[re.sub(\"_\", \" \", e.strip()) for e in triple.get_text().split(\"|\")] for triple in entry.select(\"modifiedtripleset > mtriple\")]\n",
    "        spo_list = [{\"subject\": spo[0], \"predicate\": spo[1], \"object\": spo[2]} for spo in spo_list]\n",
    "        for sent in sents:\n",
    "            if sent not in sent2spo_list:\n",
    "                sent2spo_list[sent] = spo_list\n",
    "            else:\n",
    "                sent2spo_list[sent].extend(spo_list)\n",
    "    for sent, spo_list in sent2spo_list.items():\n",
    "        # filter duplicates\n",
    "        spo_memory = set()\n",
    "        filtered_spo_list = []\n",
    "        for spo in spo_list:\n",
    "            memory = \"{}\\u2E80{}\\u2E80{}\".format(spo[\"subject\"], spo[\"predicate\"], spo[\"object\"])\n",
    "            if memory in spo_memory:\n",
    "                continue\n",
    "            if spo[\"subject\"] not in sent or spo[\"object\"] not in sent:\n",
    "                continue\n",
    "            filtered_spo_list.append(spo)\n",
    "            spo_memory.add(memory)\n",
    "        if len(filtered_spo_list) == 0:\n",
    "            continue\n",
    "        normal_sample_list.append({\n",
    "            \"text\": sent,\n",
    "            \"relation_list\": filtered_spo_list,\n",
    "        })\n",
    "    return normal_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading entries: 100%|██████████| 52/52 [00:10<00:00,  5.12it/s]\n",
      "Transforming into normal format: 100%|██████████| 6940/6940 [00:07<00:00, 936.73it/s] \n",
      "Loading entries: 100%|██████████| 52/52 [00:02<00:00, 19.11it/s]\n",
      "Transforming into normal format: 100%|██████████| 872/872 [00:00<00:00, 948.46it/s] \n"
     ]
    }
   ],
   "source": [
    "train_normal_sample_list = get_normal_sample_list(webnlg_train_data_file_path_list)\n",
    "valid_normal_sample_list = get_normal_sample_list(webnlg_valid_data_file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12995\n",
      "1671\n"
     ]
    }
   ],
   "source": [
    "print(len(train_normal_sample_list))\n",
    "print(len(valid_normal_sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12995/12995 [00:00<00:00, 842361.19it/s]\n",
      "100%|██████████| 1671/1671 [00:00<00:00, 752165.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# add text id\n",
    "text_id = 0\n",
    "for sample in tqdm(train_normal_sample_list):\n",
    "    sample[\"id\"] = text_id\n",
    "    text_id += 1\n",
    "    \n",
    "for sample in tqdm(valid_normal_sample_list):\n",
    "    sample[\"id\"] = text_id\n",
    "    text_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14666/14666 [00:02<00:00, 6303.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 0\n",
    "for sample in tqdm(train_normal_sample_list + valid_normal_sample_list):\n",
    "    sent = sample[\"text\"]\n",
    "    seq_len = len(tokenizer.tokenize(sent))\n",
    "    max_seq_len = max(max_seq_len, seq_len)\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "webnlg_data_dir = os.path.join(data_home, \"webnlg\", \"pre_by_yubowen\")\n",
    "webnlg_train_data_path = os.path.join(webnlg_data_dir, \"train.json\")\n",
    "webnlg_valid_data_path = os.path.join(webnlg_data_dir, \"dev.json\")\n",
    "webnlg_test_data_path = os.path.join(webnlg_data_dir, \"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_train_data = json.load(open(webnlg_train_data_path, \"r\", encoding = \"utf-8\"))\n",
    "web_valid_data = json.load(open(webnlg_valid_data_path, \"r\", encoding = \"utf-8\"))\n",
    "web_test_data = json.load(open(webnlg_test_data_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_spo_list = []\n",
    "bad_sample_list = []\n",
    "total_spo_num = 0\n",
    "for sample in web_train_data + web_valid_data + web_test_data:\n",
    "    bad_sample = False\n",
    "    for spo in sample[\"spo_list\"]:\n",
    "        total_spo_num += 1\n",
    "        if \"``\" in spo:\n",
    "            bad_spo_list.append(spo)\n",
    "            bad_sample = True\n",
    "    if bad_sample:\n",
    "        bad_sample_list.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 127 14144\n"
     ]
    }
   ],
   "source": [
    "print(len(bad_sample_list), len(bad_spo_list), total_spo_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6222/6222 [00:00<00:00, 6367.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 0\n",
    "for sample in tqdm(web_train_data + web_valid_data + web_test_data):\n",
    "    text = \" \".join(sample[\"tokens\"])\n",
    "    max_seq_len = max(max_seq_len, len(tokenizer.tokenize(text)))\n",
    "    spo_list = sample[\"spo_list\"]\n",
    "    for spo in spo_list:\n",
    "        if spo[0] not in text or spo[2] not in text:\n",
    "            se_trace()\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rel_set = set([spo[1] for sample in web_train_data for spo in sample[\"spo_list\"]])\n",
    "valid_rel_set = set([spo[1] for sample in web_valid_data for spo in sample[\"spo_list\"]])\n",
    "test_rel_set = set([spo[1] for sample in web_test_data for spo in sample[\"spo_list\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in tqdm(web_train_data):\n",
    "#     for spo in sample[\"spo_list\"]:\n",
    "#         if spo[1] == \"leader\":\n",
    "#             print(spo)\n",
    "#             print(\" \".join(sample[\"tokens\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG by CasRel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "webnlg_data_dir = os.path.join(data_home, \"webnlg\", \"pre_by_casrel\", \"triples\")\n",
    "webnlg_train_data_path = os.path.join(webnlg_data_dir, \"train_triples.json\")\n",
    "webnlg_valid_data_path = os.path.join(webnlg_data_dir, \"valid_triples.json\")\n",
    "webnlg_test_data_path = os.path.join(webnlg_data_dir, \"test_triples.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_train_data = json.load(open(webnlg_train_data_path, \"r\", encoding = \"utf-8\"))\n",
    "web_valid_data = json.load(open(webnlg_valid_data_path, \"r\", encoding = \"utf-8\"))\n",
    "web_test_data = json.load(open(webnlg_test_data_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Alan Bean , who was part of Apollo 12 , was born in Wheeler , Texas on March 15th , 1932 and is now retired .',\n",
       " 'triple_list': [['Bean', 'was a crew member of', '12'],\n",
       "  ['Bean', 'birthPlace', 'Texas']]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6222/6222 [00:01<00:00, 5888.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 0\n",
    "for sample in tqdm(web_train_data + web_valid_data + web_test_data):\n",
    "    text = sample[\"text\"]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    max_seq_len = max(max_seq_len, len(tokens))\n",
    "max_seq_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
